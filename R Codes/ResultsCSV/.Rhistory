# Codes for the Captons Coursera Project
# Libraries
library(ggplot2)
library(tm)
library(graph)
library(Rgraphviz)
library(wordcloud)
library(topicmodels)
library(RWeka)
library(dplyr)
# Change work directory
directory<-'/home/yasel/CursoDS/Captons'
source("/home/yasel/Dropbox/Data Science/Capston/Functions.R")
if (dir.exists(paths=directory)){
setwd(directory)
}else{
dir.create(file.path('/home/yasel/CursoDS/Captons'))
setwd(directory)
}
# Training data to get you started that will be the basis for most of the capstone.
# You must download the data from the link below and not from external websites to start.
link<-"https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
nameFile<-'Coursera-SwiftKey.zip'
# Download file
if (!file.exists(nameFile)){
download.file(link,nameFile)
}
# Unzip the file
if (!dir.exists('final')){
unzip(nameFile)
}
Twitter<-load('final.US_Twitter.RData'); Twitter<-datos
Blogs<-load('final.US.blogs.RData'); Blogs<-datos
News<-load('final.US.news.RData'); News<-datos
docs<-c(Twitter,Blogs,News)
corpus<-createCorpus(docs)
####################################################################
######################## MODEL ANALYSIS ############################
####################################################################
## Model
# Other approach ------------------------------
# Merge the corpus in one single  vector
vectorCorpus<-unlist(corpus,use.names=FALSE)
# create vector of words (anything separated by a space)
t_vec <- unlist(strsplit(vectorCorpus, '\\s'),use.names=FALSE)
# Avoid blank spaces
t_vec<-t_vec[!t_vec==""]
0.7046756/0.015182801
sqrt(90)
sqrt(150)
sqrt(987)
knitr::opts_chunk$set(echo = TRUE)
library(xlsx)
library(dplyr)
library(plotly)
library(gridExtra)
library(scales)
setwd('/home/yasel/Dropbox/Yasel-Paloma/DataPaloma/Virus_VS_all_Genes/')
source("/home/yasel/TRABAJO/Doctorado/Adenovirus/GenStudy/RCode/Functions.R")
ficheros <- dir()
E1BPM<-read.xlsx(ficheros[1], 1)
E1B<-read.xlsx(ficheros[2], 1)
WT<-read.xlsx(ficheros[3], 1)
E1BPM<-read.xlsx(ficheros[1], 1)
ficheros <- dir()
ficheros
setwd('/home/yasel/Dropbox/Yasel-Paloma/DataPaloma/Virus_VS_all_Genes/')
source("/home/yasel/TRABAJO/Doctorado/Adenovirus/GenStudy/RCode/Functions.R")
ficheros <- dir()
ficheros
ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
group <- gl(2, 10, 20, labels = c("Ctl","Trt"))
weight <- c(ctl, trt)
lm.D9 <- lm(weight ~ group)
lm.D9$residuals
summary(lm)
summary(lm.D9)
1/(length(lm.D9$residuals)-2)*sum(lm.D9$residuals.^2)
sum(lm.D9$residuals.^2)
sum(lm.D9$residuals)
(lm.D9$residuals.^2)
(lm.D9$residuals^2)
sum(lm.D9$residuals^2)
(1/(length(lm.D9$residuals)-2))*sum(lm.D9$residuals^2)
(1/(length(lm.D9$residuals)-1))*sum(lm.D9$residuals^2)
summary(lm.D9)
su<-summary(lm.D9)
su$sigma
source("/home/yasel/TRABAJO/Doctorado/Adenovirus/GenStudy/RCode/Functions.R")
word<-"conditional conditional conditional"
final<-conditionalProb(word,t_vec)
word<-"conditional"
final<-conditionalProb(word,t_vec)
# Codes for the Captons Coursera Project
# Libraries
library(ggplot2)
library(tm)
library(graph)
library(Rgraphviz)
library(wordcloud)
library(topicmodels)
library(RWeka)
library(dplyr)
# Change work directory
directory<-'/home/yasel/CursoDS/Captons'
source("/home/yasel/Dropbox/Data Science/Capston/Functions.R")
if (dir.exists(paths=directory)){
setwd(directory)
}else{
dir.create(file.path('/home/yasel/CursoDS/Captons'))
setwd(directory)
}
# Training data to get you started that will be the basis for most of the capstone.
# You must download the data from the link below and not from external websites to start.
link<-"https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
nameFile<-'Coursera-SwiftKey.zip'
# Download file
if (!file.exists(nameFile)){
download.file(link,nameFile)
}
# Unzip the file
if (!dir.exists('final')){
unzip(nameFile)
}
#--------------------------------------------------------------
## Create a training sub-dataset. Only is necessary to run this code one time.
## Open the connection
if(!file.exists('final.US_Twitter.RData')){
loadFilter("final/en_US/en_US.twitter.txt",'final.US_Twitter.RData')
loadFilter("final/en_US/en_US.blogs.txt",'final.US.blogs.RData')
loadFilter("final/en_US/en_US.news.txt",'final.US.news.RData')
}else{
Twitter<-load('final.US_Twitter.RData'); Twitter<-datos
Blogs<-load('final.US.blogs.RData'); Blogs<-datos
News<-load('final.US.news.RData'); News<-datos
}
#--------------------------------------------------------------
# Paste the sentences of the same document
# Twitter<-paste(Twitter,collapse = " ")
# Blogs<-paste(Blogs,collapse = " ")
# News<-paste(News,collapse = " ")
docs<-c(Twitter,Blogs,News)
corpus<-createCorpus(docs)
####################################################################
######################## MODEL ANALYSIS ############################
####################################################################
## Model
# Other approach ------------------------------
# Merge the corpus in one single  vector
vectorCorpus<-unlist(corpus,use.names=FALSE)
# create vector of words (anything separated by a space)
t_vec <- unlist(strsplit(vectorCorpus, '\\s'),use.names=FALSE)
# Avoid blank spaces
t_vec<-t_vec[!t_vec==""]
"conditional"
word<-"conditional"
final<-conditionalProb(word,t_vec)
word<-"conditional"
word<-unlist(strsplit(word,split = "\\s"))
prob<-table(t_vec)/length(t_vec)
word<-word[!is.na(as.vector(prob[word]))]
out<-getContext(t_vec,word[1],pre = 0,post=1)
out
length_word<-length(word)
setWords<-out$Matches
indices<-out$Index
indices
word
conditionalProb<-function(word,t_vec){
#' @param word: string of words.
#' @param t_vec: woords in the corpus.
# Split the string and create a vector of words
word<-unlist(strsplit(word,split = "\\s"))
prob<-table(t_vec)/length(t_vec)
word<-word[!is.na(as.vector(prob[word]))]
if (length(word)==0){
return(final=c())
}
out<-getContext(t_vec,word[1],pre = 0,post=1)
# Length of the vector "word"
length_word<-length(word)
# Matches words, index in the string of that words.
setWords<-out$Matches
indices<-out$Index
# Create the phrases
phrase<-vector(mode = "logical",length(indices))
for (i in 1:length(indices)){
vector<-t_vec[indices[i]:(indices[i]+length_word-1)]
phrase[i]<-paste(vector,collapse=" ")
}
# If no exist the full phrase in the text, reduce the dimension
# erasing the last word.
indc<-which(phrase==paste(word,collapse=" "))
t=1
while (length(indc)==0){
word<-word[1:length(word)-1]
phrase<-vector(mode = "logical",length(indices))
t=t+1
for (i in 1:length(indices)){
vector<-t_vec[indices[i]:(indices[i]+length_word-t)]
phrase[i]<-paste(vector,collapse=" ")
}
indc<-which(phrase==paste(word,collapse=" "))
}
length_word<-length(word)
pred_words<-t_vec[indices+length_word]
# Find the word that are related with all the words in "word"
# intersection<-Reduce(intersect,neighborhood)
# Unique words of "intersection"
unique_words_in_text<-unique(pred_words)
# Find the frequency of the unique variables in intersection.
freq<-table(pred_words)
freq<-freq[unique_words_in_text]
# Probability table
prob<-prob[unique_words_in_text]
# Conditional probability,
conditional<-freq/length(pred_words)
conditional<-conditional[unique_words_in_text]
final<-data.frame(row.names = unique_words_in_text,
Frequency=as.vector(freq),
Conditional_Prob=as.vector(conditional),
Word_Prob=as.vector(prob))
final<-final[order(final$Conditional_Prob,final$Word_Prob,decreasing=TRUE),]
return(final)
}
word<-"conditional"
final<-conditionalProb(word,t_vec)
if (length(final)==0){
cat("No matches in the corpus with the entry words")
}else{
head(final)
}
LMbyProtein<- function(data,Protein){
fit<-lm(y ~ x - 1,data)
t<-summary(fit)
data$predicted <- predict(fit)   # Save the predicted values
data$residuals <- residuals(fit) # Save the residual values
# Residuals graphic
# Use the residuals to make an aesthetic adjustment
# (e.g. red colour when residual in very high) to highlight points
# which are poorly predicted by the model.
Residuals<-abs(data$residuals)
res_plot<-ggplot(data, aes(x = x, y = y)) +
geom_smooth(method = "lm",formula = y~x-1, se = FALSE, color = "lightgrey") +
geom_segment(aes(xend = x, yend = predicted), alpha = .2) +
geom_point(aes(color = Residuals)) + # size also mapped
geom_point(aes(y = predicted),shape = 1)+
scale_color_continuous(low = "black", high = "red") +
guides(color = guide_colorbar())+
scale_x_continuous(breaks = seq(0,max(data$x)+0.1,by = 0.1))+
scale_y_continuous(breaks = seq(0,max(data$predicted)+0.1,by = 0.1))+
geom_label(x = min(data$x), hjust =0, y = max(data$predicted)-0.05,
label = paste("RSE=", abbreviate(as.character(t$sigma),5),"\n",
"R-squared=", abbreviate(as.character(t$r.squared),5)))+
ylab(Protein)+xlab("NSP2")+scale_fill_continuous(guide = guide_legend()) +
theme(legend.key.width = unit(2.6, 'lines'), legend.position="bottom",
axis.text.y =element_text(size=15),
axis.text.x =element_text(size=15))
# Coeff
coef<-t$coefficients
# Residuals
residual<-t$residuals
list(Res_plot=res_plot,coef=coef,res=residual)
}
library(dplyr)
library(ggplot2)
library(cowplot)
library(plotly)
setwd('/home/yasel/TRABAJO/Doctorado/Viroplasmas/Programas/R/ResultsCSV/')
dsRNA<-read.csv('NSP2rojo-dsRNAverde.csv')
PDI<-read.csv('NSP2rojo-PDIverde.csv')
VP4<-read.csv('NSP2rojo-VP4verde.csv')
VP6<-read.csv('NSP2rojo-VP6verde.csv')
VP760<-read.csv('NSP2rojo-VP760verde.csv')
VP7159<-read.csv('NSP2rojo-VP7159verde.csv')
NSP4<-read.csv('NSP4rojo-NSP2verde.csv')
NSP5<-read.csv('NSP5rojo-NSP2verde.csv')
dsRNA<-mutate(dsRNA, Protein='dsRNA')
PDI<-mutate(PDI, Protein='PDI')
VP4<-mutate(VP4, Protein='VP4')
VP6<-mutate(VP6, Protein='VP6')
VP760<-mutate(VP760, Protein='VP760')
VP7159<-mutate(VP7159, Protein='VP7159')
